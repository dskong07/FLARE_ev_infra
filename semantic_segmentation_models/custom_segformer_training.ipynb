{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install --upgrade segments-ai\n",
    "#%pip install -q transformers datasets segments-ai evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from segments.utils import get_semantic_bitmap\n",
    "from segments import SegmentsClient\n",
    "from segments.huggingface import release2dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up environment - logging into Hugging Face and Segments.ai API, defining environment variables\n",
    "For this step, make sure you have a segments.ai and hugging face account, as well as api tokens to login with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#your segments.ai api key\n",
    "api_key = \"your_api_key\"\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "client = SegmentsClient(api_key) #initializing segments.ai client\n",
    "notebook_login() #logging into HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_identifier = \"dskong07/chargers-full\" #dataset identifier on segments.ai\n",
    "name = \"chargers-labeled-full-v0.1\" #release name\n",
    "\n",
    "release_name = name #clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating publicly available repos for our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a release version from segments.ai of the usable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_release(dataset_identifier, name) #there should now be a release on my segments.ai page with the name above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping segments.ai dataset release to HF compatible dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the segments.ai datset and mapping\n",
    "release = client.get_release(dataset_identifier, release_name)\n",
    "hf_dataset = release2dataset(release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dataset output here\n",
    "ct = 0\n",
    "for sample in hf_dataset:\n",
    "    ct += 1\n",
    "\n",
    "    print(sample['name'])\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(sample['image'])\n",
    "    plt.show()\n",
    "\n",
    "    # Show the semantic segmentation label\n",
    "    semantic_bitmap = get_semantic_bitmap(sample['label.segmentation_bitmap'], sample['label.annotations'])\n",
    "    plt.imshow(semantic_bitmap)\n",
    "    plt.show()\n",
    "    \n",
    "    if (ct == 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a helper class to rename and reorganize the dataset's features to fit into the training pipeline\n",
    "\n",
    "def convert_segmentation_bitmap(d):\n",
    "    return {\n",
    "        \"label.segmentation_bitmap\":\n",
    "            get_semantic_bitmap(\n",
    "                d[\"label.segmentation_bitmap\"],\n",
    "                d[\"label.annotations\"],\n",
    "            )\n",
    "    }\n",
    "\n",
    "\n",
    "semantic_dataset = hf_dataset.map(\n",
    "    convert_segmentation_bitmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_dataset = semantic_dataset.rename_column('image', 'pixel_values')\n",
    "semantic_dataset = semantic_dataset.rename_column('label.segmentation_bitmap', 'label')\n",
    "semantic_dataset = semantic_dataset.remove_columns(['name', 'uuid', 'status', 'label.annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now pushing the dataset to HF repo\n",
    "hf_dataset_identifier = f\"dskong07/chargers-full-v0.1\"\n",
    "\n",
    "semantic_dataset.push_to_hub(hf_dataset_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating repos complete - now we should have 2 public dataset repos:\n",
    "- segments.ai dataset version 0.1 release: https://app.segments.ai/dskong07/chargers-full/releases\n",
    "- huggingface dataset of the same dataset, altered as the cell immediately above: https://huggingface.co/datasets/dskong07/chargers-full-v0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now datasets are publicly hosted, gathering the dataset and training a model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(hf_dataset_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train test\n",
    "\n",
    "ds = ds.shuffle(seed=1)\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\"\"\"\n",
    "repo_id = f\"datasets/{hf_dataset_identifier}\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"), \"r\"))\"\"\"\n",
    "\n",
    "# for some reason this isn't working, not sure where i messed up, but i'll just make a custom id2label json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a mapping for ID to human-parsable classifications\n",
    "\n",
    "id2label = {0: 'unlabeled', 1: 'screen', 2: 'body', 3: 'cable', 4: 'plug', 5: 'void-background'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now importing baseline model to be trained on the data: nvidia mit-b0 (potential for future upgrade to b1 or b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data augmentation on the training dataset to make training process more robust; here I implemented a color jitter transformer to introduce artifacts and variability in color values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import (\n",
    "    SegformerImageProcessor,\n",
    ")\n",
    "\n",
    "processor = SegformerImageProcessor()\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1) \n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring training arguments - number of training epochs, learning rate, batch size, and params such as evaluation strategy and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.00006\n",
    "batch_size = 2\n",
    "\n",
    "hub_model_id = \"segformer-b0-finetuned-segments-chargers-2-15\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"segformer-b0-finetuned-segments-chargers-outputs\",\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a method to determine training metrics - Here, we use mean Intersection over Union (IoU), using pytorch and evaluate libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "import multiprocessing\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=0,\n",
    "            reduce_labels=processor.do_reduce_labels,\n",
    "        )\n",
    "    \n",
    "    # add per category metrics as individual key-value pairs\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of one datum entry in the dataset\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that model is trained, we utilize hugging face inference API to host our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uploading the model to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model_id = \"segformer-b0-finetuned-segments-chargers-2-15\"\n",
    "kwargs = {\n",
    "    \"tags\": [\"vision\", \"image-segmentation\"],\n",
    "    \"finetuned_from\": pretrained_model_name,\n",
    "    \"dataset\": hf_dataset_identifier,\n",
    "}\n",
    "\n",
    "processor.push_to_hub(hub_model_id)\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the model repo here: https://huggingface.co/dskong07/segformer-b0-finetuned-segments-chargers-full-2-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can interact with inference via the HF hub: https://huggingface.co/docs/api-inference/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the model via HF interface to interact locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(f\"{'dskong07'}/{hub_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking raw image shape and predicted segmentation map shape in matrix form\n",
    "image = test_ds[0]['pixel_values']\n",
    "gt_seg = test_ds[0]['labels']\n",
    "image.shape, gt_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(data):\n",
    "    return (data-np.min(data))/(np.max(data)-np.min(data)) \n",
    "new_image = normalize(image)\n",
    "image.shape, new_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch import nn\n",
    "\n",
    "inputs = processor(images=new_image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "# First, rescale logits to original image size\n",
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.shape[::-1], # (height, width)\n",
    "    #scale_factor=1,\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "# Second, apply argmax on the class dimension\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# parses the matrix of (n,n) pixels identified by the model which have been categorized into classifiers (e.g 0-5 corresponding to the classification's ID, e.g. 1 = screen), and modifies each classification into a length 3 RGB value given by the palette. \n",
    "def get_seg_overlay(image, seg):\n",
    "  color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "  palette = np.array(palette)\n",
    "  for label, color in enumerate(palette):\n",
    "      color_seg[seg == label, :] = color\n",
    "\n",
    "  # Show image + mask\n",
    "  img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "  img = img.astype(np.uint8)\n",
    "\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "palette = [\n",
    "    #these are rgb values\n",
    "    [0, 0, 0],  # unlabeled \n",
    "    [216, 0, 24], # screen\n",
    "    [255, 255, 0],  #body\n",
    "    [125, 46, 141], #cable\n",
    "    [118, 171, 47], #plug\n",
    "    [125, 0, 225] #void-background\n",
    "]\n",
    "\n",
    "#overlays the raw, original image with the transformed RGB mask generated by the inference model.\n",
    "\n",
    "def get_overlays(path, is_web = True):\n",
    "\n",
    "    #url = \"https://media.wired.com/photos/6650c3c556be637959104b4c/master/w_2240,c_limit/How-Many-EV-Chargers-Do-We-Need--Gear-GettyImages-1242853407.jpg\"\n",
    "    if is_web:\n",
    "        image = Image.open(requests.get(path, stream=True).raw)\n",
    "    else:\n",
    "        image = Image.open(path)\n",
    "\n",
    "\n",
    "    #use HF interface to interact with the inference model we just trained to generate a predictive mask in results\n",
    "    \n",
    "    image_segmentator = pipeline(\n",
    "        \"image-segmentation\",\n",
    "        model=f\"{'dskong07'}/{hub_model_id}\",  # Change with your model name\n",
    "    )\n",
    "    results = image_segmentator(image)\n",
    "\n",
    "\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    segmentation_map = np.zeros_like(image_array)\n",
    "\n",
    "    for result in results:\n",
    "\n",
    "        mask = np.array(result[\"mask\"])\n",
    "\n",
    "        label = result[\"label\"]\n",
    "\n",
    "        label_index = list(id2label.values()).index(label)\n",
    "\n",
    "        color = palette[label_index]\n",
    "\n",
    "        for c in range(3):\n",
    "\n",
    "            segmentation_map[:, :, c] = np.where(mask, color[c], segmentation_map[:, :, c])\n",
    "    return image, image_array, segmentation_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example outputs on unseen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://media.wired.com/photos/6650c3c556be637959104b4c/master/w_2240,c_limit/How-Many-EV-Chargers-Do-We-Need--Gear-GettyImages-1242853407.jpg'\n",
    "image, image_array, segmentation_map = get_overlays(url)\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(image_array)\n",
    "\n",
    "plt.imshow(segmentation_map, alpha=0.5)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'example_data/example.jpg'\n",
    "image, image_array, segmentation_map = get_overlays(path, is_web = False)\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(image_array)\n",
    "\n",
    "plt.imshow(segmentation_map, alpha=0.5)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'example_data/example2.jpg'\n",
    "image, image_array, segmentation_map = get_overlays(path, is_web = False)\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(image_array)\n",
    "\n",
    "plt.imshow(segmentation_map, alpha=0.5)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

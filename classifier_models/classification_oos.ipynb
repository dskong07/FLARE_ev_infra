{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/tasks/image_classification#image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/en/image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting all images to jpg, and also renaming for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting images with broken label to png first, then converting healthy and storing them in different folders so that they can be labeled in metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '../classification_dataset/Service'\n",
    "\n",
    "# renaming and converting all files in the broken folder to png and renaming them as such.\n",
    "\n",
    "import os\n",
    "files = os.listdir(f'{fp}/notworking/')\n",
    "\n",
    "files\n",
    "ct = 1\n",
    "for i in files:\n",
    "    im = Image.open(f'{fp}/notworking/{i}').convert(\"RGB\")\n",
    "    im.save(f\"{fp}/notworking_conv/broken{ct}.png\", \"png\")\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = os.listdir(f'{fp}/working/')\n",
    "# renaming and converting all files in the healthy folder to png and renaming them as such.\n",
    "\n",
    "files\n",
    "ct = 1\n",
    "for i in files:\n",
    "    im = Image.open(f'{fp}/working/{i}').convert(\"RGB\")\n",
    "    im.save(f\"{fp}/working_conv/healthy{ct}.png\", \"png\")\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "healthy = os.listdir(f'{fp}/working_conv/')\n",
    "broken = os.listdir(f'{fp}/notworking_conv/')\n",
    "#with open('screen_metadata.csv', newline='') as csvfile:\n",
    "f = open('charger_metadata.csv','w')\n",
    "    #csvfile.write('file_name,broken')\n",
    "f.write('file_name,label')\n",
    "f.write('\\n')\n",
    "for img in healthy:\n",
    "        f.write(f'{img}, 0')\n",
    "        f.write('\\n')\n",
    "for img in broken:\n",
    "        f.write(f'{img}, 1')\n",
    "        f.write('\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ..._metadata.csv will be shortened to metadata.csv in order to align with huggingface's dataset upload requirements, and be put into a folder called \"cord_dataset\", along with the newly named and labeled images manually, which is then uploaded using the huggingface client, shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=f\"{fp}/service_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_identifier = 'charging-charger-classification-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(f'dskong07/{dataset_identifier}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now getting from HF repo and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chargers = load_dataset(f\"dskong07/{dataset_identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chargers = chargers.shuffle(seed=1)\n",
    "ds = chargers[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images labeled with 0 in the dataset are healthy images, and 1 are broken.\n",
    "id2label={0:'healthy',1:'broken'}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we perform data augmentation on the dataset, which includes normalizing the image size such that all images are the same shape, and will also be randomly cropped to introduce variability in order with the goal of a more robust outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "\n",
    "    del examples[\"image\"]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "# we use datacollator to manage batch sizes and padding in the training process.\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the training arguments to be used in the process of creating our model. \n",
    "# We tried various batch sizes and learning rates, but the ones shown here are the parameters we felt \n",
    "# performed the best in respect to both computational efficiency and accuracy.\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"charger-classif-model\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=8,\n",
    "    #warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model inference from HF api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"charger-classif-model\")\n",
    "\n",
    "\n",
    "#classifier(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = chargers['train']['image'][0]\n",
    "img_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_img = Image.open(fp='example_data/brokencharger_example.png')\n",
    "local_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(local_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
